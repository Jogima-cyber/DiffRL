params:
  diff_env:
    name: PushCubeEnv
    stochastic_env: True
    episode_length: 1000
    MM_caching_frequency: 16

  metaworld:
    name: "push-v2-goal-observable"

  network:
    actor: SAPOActorStochasticMLP
    actor_mlp:
      units: [400, 200, 100]
      activation: silu

    critic: CriticMLP
    critic_mlp:
      units: [400, 200]
      activation: silu

    critic_adv: CriticAdvMLP
    critic_adv_mlp:
      units: [128, 128]
      activation: elu

    dyn_model: StochSSM # VAESSM
    dyn_model_mlp:
      units: [512, 512] # [1024, 1024]
      activation: silu
      reward_head_units: [512, 512] # [256, 256] # [512, 512]
      num_bins: 101
      vmin: -10.0
      vmax: 10.0
      use_bins: True
      recurrent: False
      seq_len: 25 # 50
      hidden_size: 256
      vae: False
      latent_dim: 4

  config:
    name: df_push_cube_sapo_demo
    actor_learning_rate: 5e-4 # adam
    critic_learning_rate: 5e-4 # adam
    dyn_model_learning_rate: 2e-3 # adam

    train_value_function: True
    clipped_double_critic: True # <---
    smooth_gradient: False # True # False

    lr_schedule: linear # ['constant', 'linear']
    target_critic_alpha: 0.2
    obs_rms: True
    ret_rms: False
    critic_iterations: 8 # 16 # 16
    critic_method: td-lambda # ['td-lambda', 'one-step']
    lambda: 0.95
    num_batch: 4 # 8
    gamma: 0.99
    betas: [0.7, 0.95] # adam
    max_epochs: 1000 # 1250
    steps_num: 16
    grad_norm: 1.0
    truncate_grads: True
    num_actors: 128 # 512 # 512
    save_interval: 400
    dyn_buffer_size: 2e6 # 1e6
    dyn_udt: 128
    init_dyn_udt: 10000
    min_replay: 40000
    dyn_pred_batch_size: 512
    filter_sigma_events: False # True # False
    learn_reward: True
    avantage_objective: False
    imagined_batch_size: 0

    wandb_track: True
    wandb_project_name: neuro_diff_sim

    player:
      determenistic: True
      games_num: 1
      num_actors: 1
      print_stats: True
