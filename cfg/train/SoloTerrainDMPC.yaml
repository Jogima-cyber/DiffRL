params:
  seed: ${...seed}

  algo:
    name: cat_dmpc_continuous

  #model:
  #  name: continuous_a2c_logstd

  #network:
  #  name: actor_critic
  #  separate: True

  #  space:
  #    continuous:
  #      mu_activation: None
  #      sigma_activation: None
  #      mu_init:
  #        name: default
  #      sigma_init:
  #        name: const_initializer
  #        val: 0. # std = 1.
  #      fixed_sigma: True

  #  mlp:
  #    units: [512, 256, 128]
  #    activation: elu
  #    d2rl: False

  #    initializer:
  #      name: default
  #    regularizer:
  #      name: None
    # rnn:
    #   name: lstm
    #   units: 128
    #   layers: 1
    #   before_mlp: True
    #   concat_input: True
    #   layer_norm: False


  #load_checkpoint: ${if:${...checkpoint},True,False} # flag which sets whether to load the checkpoint
  #load_path: ${...checkpoint} # path to the checkpoint to load

  config:
    name: ${resolve_default:SoloTerrain,${....experiment}}
    full_experiment_name: ${.name}
    env_name: rlgpu
    #multi_gpu: ${....multi_gpu}
    #mixed_precision: True
    #normalize_input: True
    #normalize_value: True
    #normalize_advantage: True
    #num_actors: ${....task.env.numEnvs}
    reward_shaper:
      scale_value: 0.01
    gamma: 0.99
    tau: 0.05
    critic_learning_rate: 5.e-4
    actor_learning_rate: 5.e-4
    total_timesteps: 1e8
    buffer_size: 5e6
    learning_starts: 25e3
    explo_std_min: 0.05
    explo_std_max: 0.2 # 0.1
    policy_frequency: 2
    batch_size: 8192
    policy_noise: 0.8 #0.2
    noise_clip: 0.2 # 0.5
    distributional: False
    horizon_length: 24 # 24 for t = 0.02 (decimation = 4)

    img_horizon: 16
    dynamic_pred_batch_size: 1024
    img_batch_size: 32
    target_critic_alpha: 0.2
    img_udt: 8
    dyn_udt: 256
    normalize_obs: true
    normalize_actions: true
    normalize_value: true
    dyn_lr: 3e-4
    min_replay: 25000
    init_dyn_udt: 100
    img_bptt_horizon: 1
    pretrained_wm: true
    pretrained_wm_path: "runs/SoloTerrainDMPC_pretrain_WM_1720720408"

    max_epochs: ${resolve_default:1000,${....max_iterations}}
    #save_best_after: 250
    #score_to_win: 2000000000
    #save_frequency: 250
    #print_stats: False

    #player:
    #  deterministic: True
    #  use_vecenv: True
    #  games_num: 2000
    #  print_stats: False
